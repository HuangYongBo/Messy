1创建一个scrapy项目:
#scrapy startobject First
	生成目录树:
	First
	   ├── First
	   │   ├── __init__.py   可以倒入包
	   │   ├── items.py 	 爬虫提取的数据
	   │   ├── middlewares.py	中间件(加代理之类的东西),旧版本不存在
	   │   ├── pipelines.py   管道文件,用于处理items数据,处理每一个items文件
	   │   ├── settings.py	  项目执行参考的配置文件
	   │   └── spiders
	   │       └── __init__.py	
	   └── scrapy.cfg  配置信息

	settings.py中:
	BOT_NAME:项目名
	SPIDIR_MOULES:爬虫的路径(First.spiders)
	NEWSPIDIR_MOULES:新的爬虫存放的路径(同上)
	USER_AGENT:
	ROBOTSTXT_OBEY:是否遵循网站的协议(有些网站不允许爬虫)
	CONCURRENT_REQUESTS:请求并发量,一次发送请求的个数
	DOWNLOAD_DELAY:等待延迟
	CONCURRENT_REQUESTS_PER_DOMAIN:允许域的个数
	CONCURRENT_REQUESTS_PER_IP:允许的IP
	COOKIES_ENABLED:是否启用cookie
	DEFAULT_REQUEST_HEADERS:默认请求报头
	SPIDER_MIDDLEWARES:中间件
	DOWNLOADDER_MIDDLEWARES:中间件
	EXTENSIONS:配置监控
	ITEM_PIPELINES:管道文件
2生成一个spider文件:
scrapy genspider first "www.itcast.cn" (最后一个参数是域范围,链接超过域的范围则不爬取)


